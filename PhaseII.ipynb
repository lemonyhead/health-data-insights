{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dp7Pm-Suh3d"
   },
   "source": [
    "## Data Sources\n",
    "*   Downloaded Dataset Source: https://catalog.data.gov/dataset/heart-disease-mortality-data-among-us-adults-35-by-state-territory-and-county\n",
    "*   Web Collection #1 Source: https://en.wikipedia.org/wiki/Obesity_in_the_United_States\n",
    "*   Web Collection #2 Source: https://www.healthit.gov/data/open-api?source=Surescripts_04-2014_State.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as random\n",
    "import pandas as pd\n",
    "import json, csv, requests, re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mRjxZDbE1tj"
   },
   "source": [
    "## Downloaded Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "executionInfo": {
     "elapsed": 685,
     "status": "error",
     "timestamp": 1617421302487,
     "user": {
      "displayName": "Bohong Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9YrzBjDpMd1KMv6KhO1wXCLX0XnbNKeDk0tkAgA=s64",
      "userId": "07627097596832814931"
     },
     "user_tz": 240
    },
    "id": "0p5xxmqzFGrO",
    "outputId": "efbb7221-19b9-4bb7-e5d3-302f144b257f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Avg_Num_Deaths  Percent Total %\n",
      "LocationAbbr                                 \n",
      "AK                318.396517         1.698614\n",
      "AL                452.940996         2.416396\n",
      "AR                447.272996         2.386157\n",
      "AS                343.533333         1.832716\n",
      "AZ                272.962357         1.456227\n",
      "CA                298.068489         1.590166\n",
      "CO                247.797399         1.321975\n",
      "CT                255.655882         1.363899\n",
      "DC                306.178571         1.633432\n",
      "DE                270.705172         1.444185\n",
      "FL                286.450326         1.528184\n",
      "GA                380.160799         2.028120\n",
      "GU                529.800000         2.826431\n",
      "HI                300.981707         1.605708\n",
      "IA                329.146016         1.755962\n",
      "ID                278.923204         1.488028\n",
      "IL                343.175380         1.830807\n",
      "IN                354.644851         1.891995\n",
      "KS                339.465344         1.811014\n",
      "KY                431.482062         2.301914\n",
      "LA                452.033086         2.411552\n",
      "MA                235.640385         1.257118\n",
      "MD                305.678086         1.630762\n",
      "ME                306.322321         1.634199\n",
      "MI                373.413155         1.992122\n",
      "MN                263.462181         1.405545\n",
      "MO                406.578002         2.169054\n",
      "MP                379.133333         2.022639\n",
      "MS                495.518182         2.643541\n",
      "MT                349.521053         1.864660\n",
      "NC                318.169751         1.697404\n",
      "ND                336.993575         1.797827\n",
      "NE                299.904294         1.599960\n",
      "NH                250.927451         1.338673\n",
      "NJ                287.223123         1.532307\n",
      "NM                299.150000         1.595936\n",
      "NV                342.786873         1.828734\n",
      "NY                324.855941         1.733074\n",
      "OH                358.969450         1.915067\n",
      "OK                451.891615         2.410797\n",
      "OR                252.234354         1.345645\n",
      "PA                333.401166         1.778662\n",
      "PR                222.775527         1.188485\n",
      "RI                260.948276         1.392133\n",
      "SC                349.207394         1.862987\n",
      "SD                336.514409         1.795271\n",
      "TN                424.682379         2.265639\n",
      "TX                362.380342         1.933263\n",
      "US                301.700000         1.609540\n",
      "UT                273.006774         1.456464\n",
      "VA                327.246877         1.745830\n",
      "VI                 88.300000         0.471072\n",
      "VT                305.188421         1.628150\n",
      "WA                271.518182         1.448523\n",
      "WI                309.550390         1.651421\n",
      "WV                386.078851         2.059693\n",
      "WY                313.842512         1.674319\n"
     ]
    }
   ],
   "source": [
    "def data_parser():\n",
    "    #import pandas as pd  #already imported delete later\n",
    "    df = pd.read_csv('Heart_Disease_Mortality.csv')\n",
    "\n",
    "    #removes the columns\n",
    "    df.drop('Year', inplace=True, axis=1)\n",
    "    df.drop('GeographicLevel', inplace=True, axis=1)\n",
    "    df.drop('DataSource', inplace=True, axis=1)\n",
    "    df.drop('Class', inplace=True, axis=1)\n",
    "    df.drop('Topic', inplace=True, axis=1)\n",
    "    df.drop('Data_Value_Unit', inplace=True, axis=1)\n",
    "    df.drop('Data_Value_Type', inplace=True, axis=1)\n",
    "    df.drop('Data_Value_Footnote_Symbol', inplace=True, axis=1)\n",
    "    df.drop('Data_Value_Footnote', inplace=True, axis=1)\n",
    "    df.drop('StratificationCategory1', inplace=True, axis=1)\n",
    "    df.drop('Stratification1', inplace=True, axis=1)\n",
    "    df.drop('StratificationCategory2', inplace=True, axis=1)\n",
    "    df.drop('Stratification2', inplace=True, axis=1)\n",
    "    df.drop('TopicID', inplace=True, axis=1)\n",
    "    df.drop('LocationID', inplace=True, axis=1)\n",
    "    df.drop('Location 1', inplace=True, axis=1)\n",
    "    df.drop('LocationDesc', inplace=True, axis=1)\n",
    "\n",
    "    #getting rid of the columns with np.nan in the 'Data_Value' column\n",
    "    df = df[df['Data_Value'].notna()]\n",
    "\n",
    "    #sorting the datavalues by 'LocationAbbr'\n",
    "    df.sort_values(by=['LocationAbbr'], ascending=True, inplace = True)\n",
    "\n",
    "    #resets the index of each row to increase from 0 to the number of rows\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    #Finding the average number of deaths due to Heart Disease per state/territory per 100,000 people\n",
    "    df = df.groupby(['LocationAbbr']).agg({'Data_Value': 'mean'})\n",
    "\n",
    "    #Rename the Data_Value column to Avg_Num_Deaths\n",
    "    df.rename(columns = {'Data_Value':'Avg_Num_Deaths'}, inplace = True)\n",
    "\n",
    "    #Finding the total number of deaths within in the US states and territories, this can be used to find the percentage that each state makes up\n",
    "    total = 0\n",
    "    for row in df['Avg_Num_Deaths']:\n",
    "        total += row\n",
    "\n",
    "    #Adding a new column to df that represents the percentage of heart disease deaths in the country that fall within the state\n",
    "    percentage  = []\n",
    "    for row in df['Avg_Num_Deaths']:\n",
    "        percentage.append(row/total*100)\n",
    "    df['Percent Total %'] = percentage\n",
    "\n",
    "    #Writing the dataframe to an output CSV file\n",
    "    df.to_csv('heart_disease_output.csv', index=True)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "############ Function Call ############\n",
    "data_parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "794L4vGXFdYw"
   },
   "source": [
    "## Web Collection \\#1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "vXwpJObDFiWM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['States, district, & territories', 'Obesity rank', 'Obese adults (mid-2000s)', 'Obese adults (2020)', 'Overweight (incl. obese) adults(mid-2000s)', 'Obese children and adolescents(mid-2000s)'], ['Alabama', '5', '30.1%', '36.3%', '65.4%', '16.7%'], ['Alaska', '9', '27.3%', '34.2%', '64.5%', '11.1%'], ['American Samoa', 'n/a', 'n/a', '75%', '95%', '35%'], ['Arizona', '30', '23.3%', '29.5%', '59.5%', '12.2%'], ['Arkansas', '7', '28.1%', '35.0%', '64.7%', '16.4%'], ['California', '48', '23.1%', '25.1%', '59.4%', '13.2%'], ['Colorado', '51', '21.0%', '22.6%', '55.0%', '9.9%'], ['Connecticut', '42', '20.8%', '26.9%', '58.7%', '12.3%'], ['Delaware', '23', '25.9%', '31.8%', '63.9%', '22.8%'], ['District of Columbia', '50', '22.1%', '23.0%', '55.0%', '14.8%'], ['Florida', '35', '23.3%', '28.4%', '60.8%', '14.4%'], ['Georgia', '24', '27.5%', '31.6%', '63.3%', '16.4%'], ['Guam', 'n/a', 'n/a', '28.3%', 'n/a', '22%'], ['Hawaii', '49', '20.7%', '23.8%', '55.3%', '13.3%'], ['Idaho', '32', '24.6%', '29.3%', '61.4%', '10.1%'], ['Illinois', '27', '25.3%', '31.1%', '61.8%', '15.8%'], ['Indiana', '12', '27.5%', '33.6%', '62.8%', '15.6%'], ['Iowa', '4', '26.3%', '36.4%', '63.4%', '12.5%'], ['Kansas', '18', '25.8%', '32.4%', '62.3%', '14.0%'], ['Kentucky', '8', '28.4%', '34.3%', '66.8%', '20.6%'], ['Louisiana', '6', '29.5%', '36.2%', '64.2%', '17.2%'], ['Maine', '33', '23.7%', '29.1%', '60.8%', '12.7%'], ['Maryland', '26', '25.2%', '31.3%', '61.5%', '13.3%'], ['Massachusetts', '44', '20.9%', '25.9%', '56.8%', '13.6%'], ['Michigan', '19', '27.7%', '32.3%', '63.9%', '14.5%'], ['Minnesota', '35', '24.8%', '28.4%', '61.9%', '10.1%'], ['Mississippi', '2', '34.4%', '37.3%', '67.4%', '17.8%'], ['Missouri', '17', '27.4%', '32.5%', '63.3%', '15.6%'], ['Montana', '46', '21.7%', '25.3%', '59.6%', '11.1%'], ['Nebraska', '15', '26.5%', '32.8%', '63.9%', '11.9%'], ['Nevada', '43', '23.6%', '26.7%', '61.8%', '12.4%'], ['New Hampshire', '38', '23.6%', '28.1%', '60.8%', '12.9%'], ['New Jersey', '41', '22.9%', '27.3%', '60.5%', '13.7%'], ['New Mexico', '35', '23.3%', '28.4%', '60.3%', '16.8%'], ['New York', '45', '23.5%', '25.7%', '60.0%', '15.3%'], ['North Carolina', '20', '27.1%', '32.1%', '63.4%', '19.3%'], ['North Dakota', '13', '25.9%', '33.2%', '64.5%', '12.1%'], ['Northern Mariana Islands', 'n/a', 'n/a', 'n/a', 'n/a', '16%'], ['Ohio', '11', '26.9%', '33.8%', '63.3%', '14.2%'], ['Oklahoma', '3', '28.1%', '36.5%', '64.2%', '15.4%'], ['Oregon', '31', '25.0%', '29.4%', '60.8%', '14.1%'], ['Pennsylvania', '24', '25.7%', '31.6%', '61.9%', '13.3%'], ['Puerto Rico', 'n/a', 'n/a', '30.7%', 'n/a', '26%'], ['Rhode Island', '29', '21.4%', '30.0%', '60.4%', '11.9%'], ['South Carolina', '10', '29.2%', '34.1%', '65.1%', '18.9%'], ['South Dakota', '22', '26.1%', '31.9%', '64.2%', '12.1%'], ['Tennessee', '15', '29.0%', '32.8%', '65.0%', '20.0%'], ['Texas', '14', '27.2%', '33.0%', '64.1%', '19.1%'], ['Utah', '46', '21.8%', '25.3%', '56.4%', '8.5%'], ['Vermont', '40', '21.1%', '27.6%', '56.9%', '11.3%'], ['Virgin Islands (U.S.)', 'n/a', 'n/a', '32.5%', 'n/a', 'n/a'], ['Virginia', '28', '25.2%', '30.1%', '61.6%', '13.8%'], ['Washington', '39', '24.5%', '27.7%', '60.7%', '10.8%'], ['West Virginia', '1', '30.6%', '38.1%', '66.8%', '20.9%'], ['Wisconsin', '21', '25.5%', '32.0%', '62.4%', '13.5%'], ['Wyoming', '34', '24.0%', '28.8%', '61.7%', '8.7%']]\n"
     ]
    }
   ],
   "source": [
    "def web_parser1():\n",
    "    #Url of the website with the table\n",
    "    r = requests.get(\"https://en.wikipedia.org/wiki/Obesity_in_the_United_States\")\n",
    "\n",
    "    #Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "\n",
    "    #Find the table tag\n",
    "    table = soup.find('table',{'class':'wikitable sortable'})\n",
    "\n",
    "    #This is the list where the final data will be appended\n",
    "    output_list = []\n",
    "\n",
    "    #Find the header row and getting the column headers (tr), appending it to the final list\n",
    "    header_row = table.find('tr')\n",
    "    headers = header_row.find_all('th')\n",
    "    header_info = []\n",
    "    for header in headers:\n",
    "        info = (header.get_text().strip())\n",
    "        if \"[\" in info:\n",
    "            info = info[:-4]\n",
    "        if \"[\" in info:\n",
    "            info = info[:-4]\n",
    "        header_info.append(info)\n",
    "    output_list.append(header_info)\n",
    "\n",
    "    #Find all row tags (tr) in the table\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    #This is the regex match text\n",
    "    regex_replace = re.compile(r'\\[.*?\\]')\n",
    "\n",
    "    #Iterate over each row and getting the data from each cell (td)\n",
    "    for row in rows[1:]: #skipping the header row which would otherwise be blank\n",
    "        info = row.find_all('td')\n",
    "        row_data = []\n",
    "        for item in info:\n",
    "            cleaned_data = (item.get_text().strip())\n",
    "            if \"[\" in cleaned_data:\n",
    "                cleaned_data = cleaned_data[:-4]\n",
    "            if \"[\" in cleaned_data:\n",
    "                cleaned_data = cleaned_data[:-4]\n",
    "            row_data.append(cleaned_data)\n",
    "        output_list.append(row_data)\n",
    "\n",
    "    #Replace the '-' with n/a to better represent the meaning of '-'\n",
    "    for lists in output_list:\n",
    "        for i in range(len(lists)):\n",
    "            if lists[i] == '—':\n",
    "                lists[i] = \"n/a\"\n",
    "    \n",
    "    #Writing the output_list to an output_file using csv\n",
    "    #opening an output file for writing named US_Obesity\n",
    "    with open('US_Obesity.csv', 'w', newline='') as output:\n",
    "        writer = csv.writer(output)\n",
    "        writer.writerows(output_list)\n",
    "    \n",
    "    print(output_list)\n",
    "\n",
    "############ Function Call ############\n",
    "web_parser1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDD6sMsCXRxc"
   },
   "source": [
    "## Web Collection \\#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "HAkUOqMgXQJG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       tot_e_Rx  pct_pharm_enabled  pct_pharm_e_Rx\n",
      "region_code period                                                \n",
      "AK          2008     113.000000           0.780000        0.710000\n",
      "            2009     280.000000           0.890000        0.830000\n",
      "            2010     386.500000           0.890000        0.885000\n",
      "            2011     551.583333           0.925000        0.923333\n",
      "            2012     765.416667           0.940000        0.939167\n",
      "...                         ...                ...             ...\n",
      "WY          2010     259.000000           0.855000        0.830000\n",
      "            2011     423.500000           0.913333        0.905833\n",
      "            2012     662.250000           0.953333        0.950833\n",
      "            2013     900.333333           0.972500        0.972500\n",
      "            2014    1005.500000           0.975000        0.970000\n",
      "\n",
      "[367 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def web_parser2():\n",
    "    #making the initial API request\n",
    "    url = \"https://www.healthit.gov/data/open-api?source=Surescripts_04-2014_State.csv\"\n",
    "    r = requests.get(url).json()\n",
    "    \n",
    "    #removing the sub-dictionaries with the value \"National\" or null values for the key \"region\"\n",
    "    #iterating backwards because doing it forwards means that removing a sub-dictionary shifts the indices of the subsequent sub-dictionaries by 1, meaning some dictionaries that should be deleted won't be deleted\n",
    "    for num in range(len(r)-1, -1, -1):\n",
    "        if r[num][\"region\"] == \"National\" or r[num][\"region\"] == \"\":\n",
    "            r.remove(r[num])\n",
    "    \n",
    "    #removing the sub-dictionaries with only the year and no month for the \"period\" key\n",
    "    #iterating backwards because doing it forwards means that removing a sub-dictionary shifts the indices of the subsequent sub-dictionaries by 1, meaning some dictionaries that should be deleted won't be deleted\n",
    "    for num in range(len(r)-1, -1, -1):\n",
    "        match_object = re.match(\"[0-9]{4}-[0-9]{2}\", r[num][\"period\"]) #\"period\" key is in a \"YYYY-MM\" format for periods including month and year and \"YYYY\" for periods including only year\n",
    "        if match_object == None:\n",
    "            r.remove(r[num])\n",
    "    \n",
    "    #replacing the remaining dates with their years only (deleting their attached months)\n",
    "    #this will make it so we can group by year when the data is converted into a pandas dataframe\n",
    "    for dictionary in r:\n",
    "        dictionary[\"period\"] = re.sub(\"[0-9]{4}-[0-9]{2}\", dictionary[\"period\"][0:4], dictionary[\"period\"])\n",
    "    \n",
    "    #removing the \"region\" key from the sub-dictionaries\n",
    "    for dictionary in r:\n",
    "        dictionary.pop(\"region\")\n",
    "        \n",
    "    #converting the json response object into a pandas dataframe\n",
    "    df_v1 = pd.DataFrame(r)\n",
    "\n",
    "    #dropping the unnecessary columns that won't be used in data analysis\n",
    "    df_v1.drop([\"tot_e_Rx_thru_ehr\", \"tot_e_Rx_thru_stand_alone\",  \"tot_primary_care_e_Rx\", \"tot_non_primary_care_e_Rx\", \"tot_nurse_prac_e_Rx\", \"tot_phys_asst_e_Rx\", \"pct_new_renewal_e_Rx\", \"tot_med_hist_reqs\",  \"tot_med_hist_req_resps\"], axis = 1, inplace = True)\n",
    "    \n",
    "    #removing the sub-dictionaries with empty strings for \"pct_pharm_enabled\" and \"pct_pharm_e_Rx\" columns\n",
    "    df_v2 = df_v1.replace(\"\", np.nan)\n",
    "    df_v2.dropna(axis = 0, inplace = True)\n",
    "    \n",
    "    #converting the types of necessary attributes from str to int/float\n",
    "    df_v2[\"tot_e_Rx\"] = df_v2[\"tot_e_Rx\"].astype(\"int\")\n",
    "    df_v2[\"pct_pharm_enabled\"] = df_v2[\"pct_pharm_enabled\"].astype(\"float\")\n",
    "    df_v2[\"pct_pharm_e_Rx\"] = df_v2[\"pct_pharm_e_Rx\"].astype(\"float\")\n",
    "    \n",
    "    #grouping the dataframe by \"region_code\" and then by \"period\", and then finding the means of the remaining columns\n",
    "    df_v3 = df_v2.groupby([\"region_code\", \"period\"]).agg({\"tot_e_Rx\": \"mean\", \"pct_pharm_enabled\": \"mean\", \"pct_pharm_e_Rx\": \"mean\"})\n",
    "    \n",
    "    #writing the resulting dataframe to a csv file\n",
    "    df_v3.to_csv(\"electronic-prescribing-adoption-use.csv\", index = True)\n",
    "    \n",
    "    print(df_v3)\n",
    "    \n",
    "############ Function Call ############\n",
    "web_parser2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttEYrm9US5s"
   },
   "source": [
    "1. CSV np.nan inconsistency: In the CSV file, after turning the file into a pandas dataframe, there were quite a few data values that registered as np.nan. With np.nan values, we would not be able to find the average values of deaths per state. To deal with this, we removed all rows that contained np.nan in the Data_Values column using masking and isna(). This left us with a dataframe with only rows that have values in the Data_Values column so we were able to actually find the averages number of deaths related to heart disease per state.\n",
    "\n",
    "2. BS4 footnote inconsistency: In the Wikipedia table, in both the header and some of the data values, there were footnotes written as \"\\[\\_\\_\\]\" where \"\\_\\_\" represents some number. This posed a lot of issues regarding using the actual data while creating visualizations. We dealt with this by using string methods to remove the “\\[\\_\\_\\]” for each item using a for loop to shorten the text while the “\\[“ was still present. This way, for phase 3 it will be easier to compare the data with the csv data and visualise it.\n",
    "\n",
    "3. JSON/API empty string inconsistency: There were empty strings for several values in the API, meaning that their corresponding json dictionaries were rendered useless for data analysis purposes. Those json dictionaries with empty strings were removed by converting the original response object to a pandas dataframe, changing the empty strings to NaN values, and then using the dropna function to drop rows with NaN values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PhaseII.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
